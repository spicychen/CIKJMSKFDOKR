import numpy as np
import random

class TDGammon:
	def __init__(self, num_inputs, num_hidden, num_output, lr=0.1, d=0.9, l=0.7):
		self.num_inputs = num_inputs
		self.num_hidden = num_hidden
		self.num_output = num_output
		self.learning_rate = lr
		self.discount_rate = d
		self.lamb = l
		
		self.hidden_layer = [0 for x in range(self.num_hidden)]
		self.output_layer = [0 for x in range(self.num_output)]
		self.prehidden_layer = None
		self.preoutput_layer = None
		
		self.input_weights = [[np.random.randn() for x in range(self.num_hidden)] for y in range(self.num_inputs)]
		self.hidden_weights = [[np.random.randn() for x in range(self.num_output)] for y in range(self.num_hidden)]
		self.input_eligibility_trace = [[0 for x in range(self.num_hidden)]	for y in range(self.num_inputs)]
		self.hidden_eligibility_trace = [[0 for x in range(self.num_output)] for y in range(self.num_hidden)]
		
	def sigmoid(self, z):
		return 1.0 / (1.0 + np.exp(-z))
		
	def feedforward(self, features):
		for j in range(self.num_hidden):
			for i in range(self.num_inputs):
				self.hidden_layer[j] += (features[i] * self.input_weights[i][j])
			self.hidden_layer[j] = self.sigmoid(self.hidden_layer[j])
			
		for j in range(self.num_output):
			for i in range(self.num_hidden):
				self.output_layer[j] += (self.hidden_layer[i] * self.hidden_weights[i][j])
			self.output_layer[j] = self.sigmoid(self.output_layer[j])
			
	def gradient(self, u):
		return u*(1.0-u)
			
	def TDlearn(self, reward):
			
		for k in range(self.num_output):
			for j in range(self.num_hidden):
				self.hidden_eligibility_trace[j][k] = (self.lamb * self.hidden_eligibility_trace[j][k] +
					gradient(self.hidden_weights[j][k]) * self.hidden_layer[j] * self.output_layer[k])
				self.hidden_weights[j][k] += self.learning_rate * 
					(reward+self.discount_rate*self.output_layer[k]-self.preoutput_layer[k]) * self.hidden_eligibility_trace[j][k]
		
		for j in range(self.num_hidden):
				for i in range(self.num_inputs):
					self.input_eligibility_trace[i][j] = (self.lamb * self.input_eligibility_trace[i][j] + 
						gradient(input_weights[i][j]) * self.input_layer[i] * self.hidden_layer[j])
					self.input_weights[i][j] += self.learning_rate * 
						(reward+self.discount_rate*self.hidden_layer[j]-self.prehidden_layer[j]) * self.input_eligibility_trace[i][j]
					